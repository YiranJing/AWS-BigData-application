{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Deploy Model Using AWS SageMaker (Batch transformation)\n",
    "Author: Yiran Jing\n",
    "\n",
    "Date: 28-06-2019\n",
    "\n",
    "\n",
    "**AWS Requirements**\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format. For this example, we use CSV. It should have the following\n",
    "- Have the predictor variable in the first column\n",
    "- Not have a header row (as per AWS requirements)\n",
    "- no customer_id (Useless column)\n",
    "- numerical entry only (cleaned dataset as per AWS requirements)\n",
    "\n",
    "**Data Description**\n",
    "- train.csv (70%) (include target)\n",
    "- validation.csv (20%) (include target) <i> : validation is the test dataset despite the confusing name </i>\n",
    "- test.csv (10%) (include target) <i> : is for evaluating the model once deployed </i>\n",
    "- test_data_Batch.csv (10%) (remove the target column from test.csv) <i> : this is test.csv with prediction col removed </i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# import useful packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import sklearn as sk \n",
    "import os.path \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.externals import joblib\n",
    "blue = sns.color_palette('Blues')[-2]\n",
    "color = sns.color_palette() \n",
    "sns.set_style('darkgrid') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_execution_role function retrieves the IAM role you created when you created your notebook instance.\n",
    "role = get_execution_role()\n",
    "# get the XGBoost container so we can run the XGBModel\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 bucket and prefix that you want to use for training and model data. \n",
    "bucket = 'taysolsdev'\n",
    "prefix = 'datasets/churn'\n",
    "\n",
    "# read in data from S3\n",
    "s3_input_train =sagemaker.s3_input(s3_data='s3://{}/{}/train/'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-28 02:05:26 Starting - Starting the training job...\n",
      "2019-06-28 02:05:29 Starting - Launching requested ML instances......\n",
      "2019-06-28 02:06:36 Starting - Preparing the instances for training......\n",
      "2019-06-28 02:07:56 Downloading - Downloading input data\n",
      "2019-06-28 02:07:56 Training - Downloading the training image...\n",
      "2019-06-28 02:08:27 Uploading - Uploading generated training model\n",
      "2019-06-28 02:08:27 Completed - Training job completed\n",
      "\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[2019-06-28:02:08:15:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[31m[2019-06-28:02:08:15:INFO] File size need to be processed in the node: 0.5mb. Available memory size in the node: 8434.87mb\u001b[0m\n",
      "\u001b[31m[2019-06-28:02:08:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[02:08:15] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[02:08:15] 4914x30 matrix with 147420 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[2019-06-28:02:08:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[02:08:15] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[02:08:15] 1404x30 matrix with 42120 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[02:08:15] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[0]#011train-error:0.203297#011validation-error:0.227208\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[1]#011train-error:0.199227#011validation-error:0.225071\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[2]#011train-error:0.204721#011validation-error:0.227208\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[3]#011train-error:0.200651#011validation-error:0.225783\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[4]#011train-error:0.19943#011validation-error:0.22792\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[5]#011train-error:0.196174#011validation-error:0.217949\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[6]#011train-error:0.195971#011validation-error:0.217236\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[7]#011train-error:0.195971#011validation-error:0.216524\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[8]#011train-error:0.192308#011validation-error:0.219373\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[9]#011train-error:0.191697#011validation-error:0.220085\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[10]#011train-error:0.187831#011validation-error:0.2151\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[11]#011train-error:0.184982#011validation-error:0.222222\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[12]#011train-error:0.183964#011validation-error:0.2151\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[13]#011train-error:0.18315#011validation-error:0.215812\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[14]#011train-error:0.183354#011validation-error:0.217236\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[15]#011train-error:0.181115#011validation-error:0.2151\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[16]#011train-error:0.180912#011validation-error:0.216524\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[17]#011train-error:0.180098#011validation-error:0.2151\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[18]#011train-error:0.179894#011validation-error:0.2151\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[19]#011train-error:0.180912#011validation-error:0.213675\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[20]#011train-error:0.179691#011validation-error:0.215812\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[21]#011train-error:0.17908#011validation-error:0.2151\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[22]#011train-error:0.177656#011validation-error:0.212251\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[23]#011train-error:0.176842#011validation-error:0.208689\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[24]#011train-error:0.176028#011validation-error:0.207265\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[25]#011train-error:0.176638#011validation-error:0.207265\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[26]#011train-error:0.176842#011validation-error:0.207977\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[27]#011train-error:0.177249#011validation-error:0.207265\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[28]#011train-error:0.175621#011validation-error:0.205128\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[29]#011train-error:0.175824#011validation-error:0.20584\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[30]#011train-error:0.174603#011validation-error:0.207977\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[31]#011train-error:0.173382#011validation-error:0.210826\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[32]#011train-error:0.173382#011validation-error:0.213675\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[33]#011train-error:0.172365#011validation-error:0.212963\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[34]#011train-error:0.170533#011validation-error:0.212963\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[35]#011train-error:0.168702#011validation-error:0.216524\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[36]#011train-error:0.169312#011validation-error:0.215812\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[37]#011train-error:0.169312#011validation-error:0.215812\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[38]#011train-error:0.169312#011validation-error:0.215812\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[39]#011train-error:0.169109#011validation-error:0.215812\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[40]#011train-error:0.168091#011validation-error:0.212963\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[41]#011train-error:0.167481#011validation-error:0.214387\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[42]#011train-error:0.16687#011validation-error:0.215812\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[43]#011train-error:0.165853#011validation-error:0.214387\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[44]#011train-error:0.165649#011validation-error:0.214387\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[45]#011train-error:0.163818#011validation-error:0.212963\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[46]#011train-error:0.164835#011validation-error:0.217236\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[47]#011train-error:0.164835#011validation-error:0.217236\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[48]#011train-error:0.165039#011validation-error:0.217236\u001b[0m\n",
      "\u001b[31m[02:08:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[49]#011train-error:0.164225#011validation-error:0.216524\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billable seconds: 51\n"
     ]
    }
   ],
   "source": [
    "#  The session object that manages interactions with Amazon SageMaker APIs and any other AWS service that the training job uses.\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# Create an instance of the sagemaker.estimator.Estimator class\n",
    "# output_path â€“ The path to the S3 bucket where Amazon SageMaker stores the training results.\n",
    "# train_instance_count: generally use only a single training instance.\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "# Set the hyperparameter values for the XGBoost training job\n",
    "xgb.set_hyperparameters(max_depth=3,\n",
    "                        verbosity=1,\n",
    "                        random_stae=960428,\n",
    "                        gamma=0,\n",
    "                        subsample=1,\n",
    "                        reg_lambda=1,\n",
    "                        silent=0, # silent must be integer, cannot be none\n",
    "                        colsample_bytree=1,\n",
    "                        min_child_weight=1,  \n",
    "                        learning_rate = 0.02,\n",
    "                        tree_method='hist',\n",
    "                        n_estimators=200,\n",
    "                        class_weight='balanced',\n",
    "                        objective='binary:logistic',#logistic regression for binary classification, output probability\n",
    "                        num_round=50 #The number of rounds for boosting (only used in the console version of XGBoost)\n",
    "                        )\n",
    "\n",
    "# start model training\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model with Batch Transform\n",
    "Batch Transform manages all necessary compute resources, including launching instances to deploy endpoints and deleting them afterward. \n",
    "\n",
    "To run a batch transform job, call the create_transform_job. method using the model that you trained before\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform\n",
    "\n",
    "Initial_instance_count :The initial number of instances to run in the Endpoint created from this Model.\n",
    "\n",
    "https://docs.aws.amazon.com/batch/latest/userguide/job_states.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the batch dataset used for prediction cannot have target column\n",
    "batch_input = 's3://taysolsdev/datasets/churn/batch/test_data_Batch.csv' # test data used for prediction\n",
    "\n",
    "batch_output = 's3://{}/{}/batch/batch-inference'.format(bucket, prefix) # specify the location of batch output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................!\n"
     ]
    }
   ],
   "source": [
    "# creates a transformer object from the trained model\n",
    "transformer = xgb.transformer(\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.m4.xlarge',\n",
    "                          output_path=batch_output)\n",
    "\n",
    "# calls that object's transform method to create a transform job\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Model Deployed with Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset with target\n",
    "test_data = 's3://taysolsdev/datasets/churn/test/test.csv'\n",
    "test_data = pd.read_csv(test_data, header=None, encoding = \"ISO-8859-1\")   # header = none \n",
    "\n",
    "# batch output based on test data\n",
    "batch_output = 's3://taysolsdev/datasets/churn/batch/batch-inference/test_data_Batch.csv.out'\n",
    "batch_output = pd.read_csv(batch_output, header=None, encoding = \"ISO-8859-1\") # header = none \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true,y_pred):\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    return precision, recall, f1, accuracy, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy     0.798\n",
      "tp         106.000\n",
      "fp          41.000\n",
      "tn         455.000\n",
      "fn         101.000\n",
      "dtype: float64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       496\n",
      "           1       0.72      0.51      0.60       207\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       703\n",
      "   macro avg       0.77      0.71      0.73       703\n",
      "weighted avg       0.79      0.80      0.79       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = test_data.iloc[:, 0]\n",
    "y_pred = np.round(batch_output) # threshold is 0.5\n",
    "\n",
    "\n",
    "#get scores\n",
    "temp_precision, temp_recall, temp_f1, temp_accuracy, tn, fp, fn, tp = get_score(y_test,y_pred)\n",
    "output = [temp_precision,temp_recall,temp_f1,temp_accuracy,tp, fp, tn, fn]\n",
    "output = pd.Series(output, index=['precision', 'recall', 'f1', 'accuracy', 'tp', 'fp', 'tn', 'fn']) \n",
    "print(output[['accuracy', 'tp', 'fp', 'tn', 'fn']])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "When we are ready to be done with this notebook, please run the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix: Another way to train model\n",
    "The version in this notebook follows the SDK format that calls to the AWS API for building an ML model. Another way to do this would be to configure the model using JSON format as below. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'CHURN-xgboost-regression-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "bucket_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "   \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": bucket_path\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.4xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        #\"max_depth\":\"3\",\n",
    "        #\"gamma\":\"0\",\n",
    "        #\"min_child_weight\":\"1\",\n",
    "        #\"silent\":\"None\",\n",
    "        \"num_round\":\"50\", \n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"class_weight\":\"balanced\",\n",
    "        \"n_estimators\":\"200\",\n",
    "        \"learning_rate\":\"0.02\",\n",
    "        \"tree_method\":\"hist\",\n",
    "        \"random_state\":\"960428\",   \n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\" : 's3://{}/{}/train/'.format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\" : 's3://{}/{}/validation/'.format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "client = boto3.client('sagemaker', region_name=region)\n",
    "client.create_training_job(**create_training_params)\n",
    "\n",
    "import time\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

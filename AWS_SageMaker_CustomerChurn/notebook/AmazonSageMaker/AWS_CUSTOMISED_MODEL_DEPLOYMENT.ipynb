{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title: Building Customised Amazon SageMaker XGBoost\n",
    "Author: Yiran Jing\n",
    "- Use Own Algorithms or Models with Amazon SageMaker\n",
    "- Automatically processing row data before making predictions \n",
    "Author: Yiran Jing\n",
    "\n",
    "Date: 02-07-2019\n",
    "\n",
    "- further steps after **[AWS_BUILTIN_MODEL_DEPLOYMENT](https://github.com/YiranJing/BigDataAnalysis/blob/master/AWS_SageMaker_CustomerChurn/notebook/AmazonSageMaker/AWS_BUILTIN_MODEL_DEPLOYMENT.ipynb)**\n",
    "- Data cleaning and Engineering details in **[Churn_Example](https://github.com/YiranJing/BigDataAnalysis/blob/master/AWS_SageMaker_CustomerChurn/notebook/ChurnDataAnalysis/Churn_Example.ipynb)**\n",
    "\n",
    "\n",
    "\n",
    "#### Dataset\n",
    "- row dataset: Telco-Customer-Churn.csv\n",
    "- clean and transformation by UDF\n",
    "\n",
    "Please note that scikit-learn XGBoost model is compatible with SageMaker XGBoost container, whereas other gradient boosted tree models (such as one trained in SparkML) are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55 µs, sys: 7 µs, total: 62 µs\n",
      "Wall time: 66.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import sklearn as sk \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## import UDF function to process row data\n",
    "from clean_transformation_churn import get_train_validation_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Amazon SageMaker role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'taysolsdev'\n",
    "prefix = 'datasets/churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install XGboost\n",
    "Note that for conda based installation, you'll need to change the Notebook kernel to the environment with conda and Python3."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda install -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the dataset\n",
    "Differ from standard AWS model that we need to split the y and x for the model training\n",
    "\n",
    "We can use pandas to read in data, Donot forget set**header=None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lmbda is: 0.25614406206807805\n"
     ]
    }
   ],
   "source": [
    "data_path = 's3://taysolsdev/datasets/Telco-Customer-Churn.csv'\n",
    "\n",
    "train_set, valid_set, test_set, batch_input = get_train_validation_test_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the batch dataset used for prediction cannot have target column\n",
    "batch_output = 's3://{}/{}/batch/batch-inference'.format(bucket, prefix) # specify the location of batch output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the XGBClassifier\n",
    "Note that in SciKit Model for AWS case, we need validation dataset for model training and **Y is the last column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and Y for datsets\n",
    "train_y = train_set.iloc[:,0] # 70% \n",
    "train_X = train_set.iloc[:,1:]\n",
    "\n",
    "valid_y = valid_set.iloc[:,0]  # 20%\n",
    "valid_X = valid_set.iloc[:,1:]\n",
    "\n",
    "test_y = test_set.iloc[:,0]  # 10%\n",
    "test_X = test_set.iloc[:,1:]\n",
    "\n",
    "# Setup xgboost model\n",
    "bt = xgb.XGBClassifier( max_depth=3,\n",
    "                        verbosity=1,\n",
    "                        random_stae=960428,\n",
    "                        gamma=0,\n",
    "                        subsample=1,\n",
    "                        reg_lambda=1,\n",
    "                        silent=0, # silent must be integer, cannot be none\n",
    "                        colsample_bytree=1,\n",
    "                        min_child_weight=1,  \n",
    "                        learning_rate = 0.02,\n",
    "                        tree_method='hist',\n",
    "                        n_estimators=200,\n",
    "                        class_weight='balanced',\n",
    "                        objective='binary:logistic') # binary classification\n",
    "\n",
    "\n",
    "bt.fit(train_X, train_y, # Train it to our data\n",
    "       eval_set=[(valid_X, valid_y)]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model file\n",
    "- Note that the model file name must satisfy the regular expression pattern:\n",
    "^\\[a-zA-Z0-9\\](-\\*\\[a-zA-Z0-9\\])\\*;\n",
    "- The model file also need to tar-zipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"DEMO-customised-xgboost-model\"\n",
    "bt._Booster.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-customised-xgboost-model\r\n"
     ]
    }
   ],
   "source": [
    "!tar czvf model.tar.gz $model_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the pre-trained model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "fObj = open(\"model.tar.gz\", 'rb')\n",
    "key= os.path.join(prefix, model_file_name, 'model.tar.gz')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model container\n",
    "This involves creating a SageMaker model from the model file previously uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads customised Model Artifacts to SageMaker \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://taysolsdev/datasets/churn/DEMO-customised-xgboost-model/model.tar.gz'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URI where a pre-trained model is stored\n",
    "model_url = 's3://{}/{}'.format(bucket,key)\n",
    "model_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train customised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The session object that manages interactions with Amazon SageMaker APIs and any other AWS service that the training job uses.\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# model_uri: URI of our pre-trained model \n",
    "customised_xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    model_uri = model_url, ## important\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "# Most hyper-parameters we have done in the pre-trained model\n",
    "customised_xgb.set_hyperparameters(num_round=50 #The number of rounds for boosting (only used in the console version of XGBoost)\n",
    "                        )\n",
    "\n",
    "# start model training\n",
    "customised_xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model with Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................!\n"
     ]
    }
   ],
   "source": [
    "# creates a transformer object from the trained model\n",
    "transformer = customised_xgb.transformer(\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.m4.xlarge',\n",
    "                          output_path=batch_output)\n",
    "\n",
    "# calls that object's transform method to create a transform job\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Model Deployed with Batch Transform\n",
    "The following same as Standard AWS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch output based on test data\n",
    "batch_output = 's3://taysolsdev/datasets/churn/batch/batch-inference/test_data_Batch.csv.out'\n",
    "batch_output = pd.read_csv(batch_output, header=None, encoding = \"ISO-8859-1\") # header = none \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true,y_pred):\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    return precision, recall, f1, accuracy, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy      0.765292\n",
      "tp           94.000000\n",
      "fp           52.000000\n",
      "tn          444.000000\n",
      "fn          113.000000\n",
      "dtype: float64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.84       496\n",
      "           1       0.64      0.45      0.53       207\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       703\n",
      "   macro avg       0.72      0.67      0.69       703\n",
      "weighted avg       0.75      0.77      0.75       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.round(batch_output) # threshold is 0.5\n",
    "\n",
    "\n",
    "#get scores\n",
    "temp_precision, temp_recall, temp_f1, temp_accuracy, tn, fp, fn, tp = get_score(test_y, pred_y)\n",
    "output = [temp_precision,temp_recall,temp_f1,temp_accuracy,tp, fp, tn, fn]\n",
    "output = pd.Series(output, index=['precision', 'recall', 'f1', 'accuracy', 'tp', 'fp', 'tn', 'fn']) \n",
    "print(output[['accuracy', 'tp', 'fp', 'tn', 'fn']])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title: Building Customised Amazon SageMaker XGBoost\n",
    "Author: Yiran Jing\n",
    "- Use Own Algorithms or Models with Amazon SageMaker\n",
    "- Automatically processing row data before making predictions \n",
    "Author: Yiran Jing\n",
    "\n",
    "Date: 01-07-2019\n",
    "\n",
    "- further steps after **[AWS_BUILTIN_MODEL_DEPLOYMENT](https://sandbox.notebook.ap-southeast-2.sagemaker.aws/notebooks/AWS_BUILTIN_MODEL_DEPLOYMENT.ipynb#Document)**\n",
    "- Data cleaning and Engineering details in **[Churn_Example](https://sandbox.notebook.ap-southeast-2.sagemaker.aws/notebooks/Churn_Example.ipynb)**\n",
    "\n",
    "\n",
    "\n",
    "#### Dataset\n",
    "- row dataset: Telco-Customer-Churn.csv\n",
    "- clean and transformation by UDF\n",
    "\n",
    "Please note that scikit-learn XGBoost model is compatible with SageMaker XGBoost container, whereas other gradient boosted tree models (such as one trained in SparkML) are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55 µs, sys: 7 µs, total: 62 µs\n",
      "Wall time: 66.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import sklearn as sk \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## import UDF function to process row data\n",
    "from clean_transformation_churn import get_train_validation_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Amazon SageMaker role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'taysolsdev'\n",
    "prefix = 'datasets/churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install XGboost\n",
    "Note that for conda based installation, you'll need to change the Notebook kernel to the environment with conda and Python3."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda install -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the dataset\n",
    "Differ from standard AWS model that we need to split the y and x for the model training\n",
    "\n",
    "We can use pandas to read in data, Donot forget set**header=None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lmbda is: 0.25614406206807805\n"
     ]
    }
   ],
   "source": [
    "data_path = 's3://taysolsdev/datasets/Telco-Customer-Churn.csv'\n",
    "\n",
    "train_set, valid_set, test_set, batch_input = get_train_validation_test_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the batch dataset used for prediction cannot have target column\n",
    "batch_output = 's3://{}/{}/batch/batch-inference'.format(bucket, prefix) # specify the location of batch output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the XGBClassifier\n",
    "Note that in SciKit Model for AWS case, we need validation dataset for model training and **Y is the last column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.227208\n",
      "[1]\tvalidation_0-error:0.227208\n",
      "[2]\tvalidation_0-error:0.227208\n",
      "[3]\tvalidation_0-error:0.227208\n",
      "[4]\tvalidation_0-error:0.227208\n",
      "[5]\tvalidation_0-error:0.227208\n",
      "[6]\tvalidation_0-error:0.227208\n",
      "[7]\tvalidation_0-error:0.227208\n",
      "[8]\tvalidation_0-error:0.227208\n",
      "[9]\tvalidation_0-error:0.227208\n",
      "[10]\tvalidation_0-error:0.227208\n",
      "[11]\tvalidation_0-error:0.227208\n",
      "[12]\tvalidation_0-error:0.227208\n",
      "[13]\tvalidation_0-error:0.227208\n",
      "[14]\tvalidation_0-error:0.227208\n",
      "[15]\tvalidation_0-error:0.227208\n",
      "[16]\tvalidation_0-error:0.227208\n",
      "[17]\tvalidation_0-error:0.225783\n",
      "[18]\tvalidation_0-error:0.225783\n",
      "[19]\tvalidation_0-error:0.225783\n",
      "[20]\tvalidation_0-error:0.225783\n",
      "[21]\tvalidation_0-error:0.225783\n",
      "[22]\tvalidation_0-error:0.225783\n",
      "[23]\tvalidation_0-error:0.225783\n",
      "[24]\tvalidation_0-error:0.225783\n",
      "[25]\tvalidation_0-error:0.225071\n",
      "[26]\tvalidation_0-error:0.225071\n",
      "[27]\tvalidation_0-error:0.225783\n",
      "[28]\tvalidation_0-error:0.225071\n",
      "[29]\tvalidation_0-error:0.225071\n",
      "[30]\tvalidation_0-error:0.225071\n",
      "[31]\tvalidation_0-error:0.224359\n",
      "[32]\tvalidation_0-error:0.224359\n",
      "[33]\tvalidation_0-error:0.224359\n",
      "[34]\tvalidation_0-error:0.224359\n",
      "[35]\tvalidation_0-error:0.224359\n",
      "[36]\tvalidation_0-error:0.224359\n",
      "[37]\tvalidation_0-error:0.224359\n",
      "[38]\tvalidation_0-error:0.224359\n",
      "[39]\tvalidation_0-error:0.224359\n",
      "[40]\tvalidation_0-error:0.224359\n",
      "[41]\tvalidation_0-error:0.224359\n",
      "[42]\tvalidation_0-error:0.224359\n",
      "[43]\tvalidation_0-error:0.224359\n",
      "[44]\tvalidation_0-error:0.224359\n",
      "[45]\tvalidation_0-error:0.224359\n",
      "[46]\tvalidation_0-error:0.224359\n",
      "[47]\tvalidation_0-error:0.224359\n",
      "[48]\tvalidation_0-error:0.224359\n",
      "[49]\tvalidation_0-error:0.224359\n",
      "[50]\tvalidation_0-error:0.224359\n",
      "[51]\tvalidation_0-error:0.224359\n",
      "[52]\tvalidation_0-error:0.224359\n",
      "[53]\tvalidation_0-error:0.224359\n",
      "[54]\tvalidation_0-error:0.224359\n",
      "[55]\tvalidation_0-error:0.224359\n",
      "[56]\tvalidation_0-error:0.224359\n",
      "[57]\tvalidation_0-error:0.224359\n",
      "[58]\tvalidation_0-error:0.224359\n",
      "[59]\tvalidation_0-error:0.224359\n",
      "[60]\tvalidation_0-error:0.224359\n",
      "[61]\tvalidation_0-error:0.224359\n",
      "[62]\tvalidation_0-error:0.224359\n",
      "[63]\tvalidation_0-error:0.224359\n",
      "[64]\tvalidation_0-error:0.224359\n",
      "[65]\tvalidation_0-error:0.224359\n",
      "[66]\tvalidation_0-error:0.225071\n",
      "[67]\tvalidation_0-error:0.225071\n",
      "[68]\tvalidation_0-error:0.225071\n",
      "[69]\tvalidation_0-error:0.225071\n",
      "[70]\tvalidation_0-error:0.225071\n",
      "[71]\tvalidation_0-error:0.225071\n",
      "[72]\tvalidation_0-error:0.225071\n",
      "[73]\tvalidation_0-error:0.225783\n",
      "[74]\tvalidation_0-error:0.225783\n",
      "[75]\tvalidation_0-error:0.225071\n",
      "[76]\tvalidation_0-error:0.222934\n",
      "[77]\tvalidation_0-error:0.222934\n",
      "[78]\tvalidation_0-error:0.222934\n",
      "[79]\tvalidation_0-error:0.222934\n",
      "[80]\tvalidation_0-error:0.222934\n",
      "[81]\tvalidation_0-error:0.222934\n",
      "[82]\tvalidation_0-error:0.222934\n",
      "[83]\tvalidation_0-error:0.222934\n",
      "[84]\tvalidation_0-error:0.223647\n",
      "[85]\tvalidation_0-error:0.224359\n",
      "[86]\tvalidation_0-error:0.224359\n",
      "[87]\tvalidation_0-error:0.224359\n",
      "[88]\tvalidation_0-error:0.225071\n",
      "[89]\tvalidation_0-error:0.224359\n",
      "[90]\tvalidation_0-error:0.223647\n",
      "[91]\tvalidation_0-error:0.222934\n",
      "[92]\tvalidation_0-error:0.223647\n",
      "[93]\tvalidation_0-error:0.223647\n",
      "[94]\tvalidation_0-error:0.223647\n",
      "[95]\tvalidation_0-error:0.223647\n",
      "[96]\tvalidation_0-error:0.223647\n",
      "[97]\tvalidation_0-error:0.223647\n",
      "[98]\tvalidation_0-error:0.223647\n",
      "[99]\tvalidation_0-error:0.222934\n",
      "[100]\tvalidation_0-error:0.222934\n",
      "[101]\tvalidation_0-error:0.223647\n",
      "[102]\tvalidation_0-error:0.222934\n",
      "[103]\tvalidation_0-error:0.222934\n",
      "[104]\tvalidation_0-error:0.224359\n",
      "[105]\tvalidation_0-error:0.224359\n",
      "[106]\tvalidation_0-error:0.225071\n",
      "[107]\tvalidation_0-error:0.223647\n",
      "[108]\tvalidation_0-error:0.223647\n",
      "[109]\tvalidation_0-error:0.225783\n",
      "[110]\tvalidation_0-error:0.225783\n",
      "[111]\tvalidation_0-error:0.225071\n",
      "[112]\tvalidation_0-error:0.225783\n",
      "[113]\tvalidation_0-error:0.226496\n",
      "[114]\tvalidation_0-error:0.225071\n",
      "[115]\tvalidation_0-error:0.225071\n",
      "[116]\tvalidation_0-error:0.225783\n",
      "[117]\tvalidation_0-error:0.225783\n",
      "[118]\tvalidation_0-error:0.225783\n",
      "[119]\tvalidation_0-error:0.225071\n",
      "[120]\tvalidation_0-error:0.225783\n",
      "[121]\tvalidation_0-error:0.223647\n",
      "[122]\tvalidation_0-error:0.22151\n",
      "[123]\tvalidation_0-error:0.220798\n",
      "[124]\tvalidation_0-error:0.222222\n",
      "[125]\tvalidation_0-error:0.220798\n",
      "[126]\tvalidation_0-error:0.22151\n",
      "[127]\tvalidation_0-error:0.220798\n",
      "[128]\tvalidation_0-error:0.220798\n",
      "[129]\tvalidation_0-error:0.22151\n",
      "[130]\tvalidation_0-error:0.220085\n",
      "[131]\tvalidation_0-error:0.220798\n",
      "[132]\tvalidation_0-error:0.220085\n",
      "[133]\tvalidation_0-error:0.220798\n",
      "[134]\tvalidation_0-error:0.218661\n",
      "[135]\tvalidation_0-error:0.218661\n",
      "[136]\tvalidation_0-error:0.220085\n",
      "[137]\tvalidation_0-error:0.219373\n",
      "[138]\tvalidation_0-error:0.220085\n",
      "[139]\tvalidation_0-error:0.220085\n",
      "[140]\tvalidation_0-error:0.220085\n",
      "[141]\tvalidation_0-error:0.220085\n",
      "[142]\tvalidation_0-error:0.220085\n",
      "[143]\tvalidation_0-error:0.220085\n",
      "[144]\tvalidation_0-error:0.220085\n",
      "[145]\tvalidation_0-error:0.220085\n",
      "[146]\tvalidation_0-error:0.220798\n",
      "[147]\tvalidation_0-error:0.218661\n",
      "[148]\tvalidation_0-error:0.22151\n",
      "[149]\tvalidation_0-error:0.220798\n",
      "[150]\tvalidation_0-error:0.222222\n",
      "[151]\tvalidation_0-error:0.22151\n",
      "[152]\tvalidation_0-error:0.222222\n",
      "[153]\tvalidation_0-error:0.220085\n",
      "[154]\tvalidation_0-error:0.219373\n",
      "[155]\tvalidation_0-error:0.220085\n",
      "[156]\tvalidation_0-error:0.219373\n",
      "[157]\tvalidation_0-error:0.220085\n",
      "[158]\tvalidation_0-error:0.219373\n",
      "[159]\tvalidation_0-error:0.219373\n",
      "[160]\tvalidation_0-error:0.217949\n",
      "[161]\tvalidation_0-error:0.218661\n",
      "[162]\tvalidation_0-error:0.218661\n",
      "[163]\tvalidation_0-error:0.217949\n",
      "[164]\tvalidation_0-error:0.218661\n",
      "[165]\tvalidation_0-error:0.217949\n",
      "[166]\tvalidation_0-error:0.218661\n",
      "[167]\tvalidation_0-error:0.218661\n",
      "[168]\tvalidation_0-error:0.218661\n",
      "[169]\tvalidation_0-error:0.218661\n",
      "[170]\tvalidation_0-error:0.217949\n",
      "[171]\tvalidation_0-error:0.215812\n",
      "[172]\tvalidation_0-error:0.218661\n",
      "[173]\tvalidation_0-error:0.217949\n",
      "[174]\tvalidation_0-error:0.217949\n",
      "[175]\tvalidation_0-error:0.217949\n",
      "[176]\tvalidation_0-error:0.215812\n",
      "[177]\tvalidation_0-error:0.217949\n",
      "[178]\tvalidation_0-error:0.215812\n",
      "[179]\tvalidation_0-error:0.215812\n",
      "[180]\tvalidation_0-error:0.215812\n",
      "[181]\tvalidation_0-error:0.215812\n",
      "[182]\tvalidation_0-error:0.218661\n",
      "[183]\tvalidation_0-error:0.216524\n",
      "[184]\tvalidation_0-error:0.218661\n",
      "[185]\tvalidation_0-error:0.218661\n",
      "[186]\tvalidation_0-error:0.218661\n",
      "[187]\tvalidation_0-error:0.216524\n",
      "[188]\tvalidation_0-error:0.216524\n",
      "[189]\tvalidation_0-error:0.215812\n",
      "[190]\tvalidation_0-error:0.217949\n",
      "[191]\tvalidation_0-error:0.215812\n",
      "[192]\tvalidation_0-error:0.217949\n",
      "[193]\tvalidation_0-error:0.220085\n",
      "[194]\tvalidation_0-error:0.220085\n",
      "[195]\tvalidation_0-error:0.218661\n",
      "[196]\tvalidation_0-error:0.2151\n",
      "[197]\tvalidation_0-error:0.2151\n",
      "[198]\tvalidation_0-error:0.2151\n",
      "[199]\tvalidation_0-error:0.2151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', class_weight='balanced',\n",
       "       colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.02, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_stae=960428,\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=0, subsample=1, tree_method='hist', verbosity=1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split X and Y for datsets\n",
    "train_y = train_set.iloc[:,0] # 70% \n",
    "train_X = train_set.iloc[:,1:]\n",
    "\n",
    "valid_y = valid_set.iloc[:,0]  # 20%\n",
    "valid_X = valid_set.iloc[:,1:]\n",
    "\n",
    "test_y = test_set.iloc[:,0]  # 10%\n",
    "test_X = test_set.iloc[:,1:]\n",
    "\n",
    "# Setup xgboost model\n",
    "bt = xgb.XGBClassifier( max_depth=3,\n",
    "                        verbosity=1,\n",
    "                        random_stae=960428,\n",
    "                        gamma=0,\n",
    "                        subsample=1,\n",
    "                        reg_lambda=1,\n",
    "                        silent=0, # silent must be integer, cannot be none\n",
    "                        colsample_bytree=1,\n",
    "                        min_child_weight=1,  \n",
    "                        learning_rate = 0.02,\n",
    "                        tree_method='hist',\n",
    "                        n_estimators=200,\n",
    "                        class_weight='balanced',\n",
    "                        objective='binary:logistic') # binary classification\n",
    "\n",
    "\n",
    "bt.fit(train_X, train_y, # Train it to our data\n",
    "       eval_set=[(valid_X, valid_y)]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model file\n",
    "- Note that the model file name must satisfy the regular expression pattern:\n",
    "^\\[a-zA-Z0-9\\](-\\*\\[a-zA-Z0-9\\])\\*;\n",
    "- The model file also need to tar-zipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"DEMO-customised-xgboost-model\"\n",
    "bt._Booster.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-customised-xgboost-model\r\n"
     ]
    }
   ],
   "source": [
    "!tar czvf model.tar.gz $model_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the pre-trained model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "fObj = open(\"model.tar.gz\", 'rb')\n",
    "key= os.path.join(prefix, model_file_name, 'model.tar.gz')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model container\n",
    "This involves creating a SageMaker model from the model file previously uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads customised Model Artifacts to SageMaker \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://taysolsdev/datasets/churn/DEMO-customised-xgboost-model/model.tar.gz'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URI where a pre-trained model is stored\n",
    "model_url = 's3://{}/{}'.format(bucket,key)\n",
    "model_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train customised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-01 04:40:43 Starting - Starting the training job...\n",
      "2019-07-01 04:40:45 Starting - Launching requested ML instances......\n",
      "2019-07-01 04:41:47 Starting - Preparing the instances for training...\n",
      "2019-07-01 04:42:44 Downloading - Downloading input data...\n",
      "2019-07-01 04:43:03 Training - Downloading the training image..\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[2019-07-01:04:43:21:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[31m[2019-07-01:04:43:21:INFO] File size need to be processed in the node: 0.5mb. Available memory size in the node: 8443.39mb\u001b[0m\n",
      "\u001b[31m[2019-07-01:04:43:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[04:43:21] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[04:43:21] 4914x30 matrix with 147420 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[2019-07-01:04:43:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[04:43:21] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[04:43:21] 1404x30 matrix with 42120 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 94 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[0]#011train-rmse:0.431361#011validation-rmse:0.441199\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[1]#011train-rmse:0.390721#011validation-rmse:0.409189\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[2]#011train-rmse:0.367108#011validation-rmse:0.393268\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 112 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[3]#011train-rmse:0.351198#011validation-rmse:0.386523\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[4]#011train-rmse:0.34189#011validation-rmse:0.383976\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 114 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[5]#011train-rmse:0.335064#011validation-rmse:0.382524\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[6]#011train-rmse:0.328794#011validation-rmse:0.381532\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 86 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[7]#011train-rmse:0.325037#011validation-rmse:0.381389\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 112 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[8]#011train-rmse:0.321119#011validation-rmse:0.38155\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 116 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[9]#011train-rmse:0.31754#011validation-rmse:0.381978\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 70 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[10]#011train-rmse:0.315161#011validation-rmse:0.382232\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[11]#011train-rmse:0.31205#011validation-rmse:0.382987\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 80 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[12]#011train-rmse:0.31059#011validation-rmse:0.382543\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 90 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[13]#011train-rmse:0.308561#011validation-rmse:0.382762\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[14]#011train-rmse:0.30726#011validation-rmse:0.383263\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[15]#011train-rmse:0.303732#011validation-rmse:0.384551\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 72 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[16]#011train-rmse:0.301291#011validation-rmse:0.384964\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 88 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[17]#011train-rmse:0.299135#011validation-rmse:0.386423\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 66 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[18]#011train-rmse:0.297094#011validation-rmse:0.386696\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 72 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[19]#011train-rmse:0.296482#011validation-rmse:0.386567\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 80 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[20]#011train-rmse:0.294336#011validation-rmse:0.386802\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 98 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[21]#011train-rmse:0.292155#011validation-rmse:0.387476\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 94 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[22]#011train-rmse:0.289935#011validation-rmse:0.387492\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 80 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[23]#011train-rmse:0.288165#011validation-rmse:0.38789\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 66 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[24]#011train-rmse:0.286762#011validation-rmse:0.388326\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[25]#011train-rmse:0.2853#011validation-rmse:0.388944\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 74 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[26]#011train-rmse:0.282914#011validation-rmse:0.389856\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[27]#011train-rmse:0.279978#011validation-rmse:0.390496\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 72 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[28]#011train-rmse:0.278705#011validation-rmse:0.391095\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[29]#011train-rmse:0.277965#011validation-rmse:0.391212\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 66 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[30]#011train-rmse:0.276581#011validation-rmse:0.391428\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 94 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[31]#011train-rmse:0.274104#011validation-rmse:0.391059\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 86 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[32]#011train-rmse:0.271903#011validation-rmse:0.391979\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[33]#011train-rmse:0.269886#011validation-rmse:0.393035\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 84 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[34]#011train-rmse:0.268649#011validation-rmse:0.393138\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[35]#011train-rmse:0.266926#011validation-rmse:0.393492\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 78 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[36]#011train-rmse:0.265084#011validation-rmse:0.393675\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 92 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[37]#011train-rmse:0.263127#011validation-rmse:0.393937\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[38]#011train-rmse:0.262296#011validation-rmse:0.394298\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 82 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[39]#011train-rmse:0.260694#011validation-rmse:0.395551\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[40]#011train-rmse:0.259771#011validation-rmse:0.395723\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[41]#011train-rmse:0.258887#011validation-rmse:0.396666\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[42]#011train-rmse:0.257652#011validation-rmse:0.397025\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 74 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[43]#011train-rmse:0.255418#011validation-rmse:0.397102\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[44]#011train-rmse:0.254951#011validation-rmse:0.39772\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 66 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[45]#011train-rmse:0.253938#011validation-rmse:0.397874\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 80 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[46]#011train-rmse:0.253236#011validation-rmse:0.397944\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 82 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[47]#011train-rmse:0.251627#011validation-rmse:0.398543\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[48]#011train-rmse:0.248796#011validation-rmse:0.399541\u001b[0m\n",
      "\u001b[31m[04:43:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[31m[49]#011train-rmse:0.246544#011validation-rmse:0.398923\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-07-01 04:43:33 Uploading - Uploading generated training model\n",
      "2019-07-01 04:43:33 Completed - Training job completed\n",
      "Billable seconds: 50\n"
     ]
    }
   ],
   "source": [
    "#  The session object that manages interactions with Amazon SageMaker APIs and any other AWS service that the training job uses.\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# model_uri: URI of our pre-trained model \n",
    "customised_xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    model_uri = model_url, ## important\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "# Most hyper-parameters we have done in the pre-trained model\n",
    "customised_xgb.set_hyperparameters(num_round=50 #The number of rounds for boosting (only used in the console version of XGBoost)\n",
    "                        )\n",
    "\n",
    "# start model training\n",
    "customised_xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model with Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................!\n"
     ]
    }
   ],
   "source": [
    "# creates a transformer object from the trained model\n",
    "transformer = customised_xgb.transformer(\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.m4.xlarge',\n",
    "                          output_path=batch_output)\n",
    "\n",
    "# calls that object's transform method to create a transform job\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Model Deployed with Batch Transform\n",
    "The following same as Standard AWS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch output based on test data\n",
    "batch_output = 's3://taysolsdev/datasets/churn/batch/batch-inference/test_data_Batch.csv.out'\n",
    "batch_output = pd.read_csv(batch_output, header=None, encoding = \"ISO-8859-1\") # header = none \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true,y_pred):\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    return precision, recall, f1, accuracy, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy      0.765292\n",
      "tp           94.000000\n",
      "fp           52.000000\n",
      "tn          444.000000\n",
      "fn          113.000000\n",
      "dtype: float64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.84       496\n",
      "           1       0.64      0.45      0.53       207\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       703\n",
      "   macro avg       0.72      0.67      0.69       703\n",
      "weighted avg       0.75      0.77      0.75       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.round(batch_output) # threshold is 0.5\n",
    "\n",
    "\n",
    "#get scores\n",
    "temp_precision, temp_recall, temp_f1, temp_accuracy, tn, fp, fn, tp = get_score(test_y, pred_y)\n",
    "output = [temp_precision,temp_recall,temp_f1,temp_accuracy,tp, fp, tn, fn]\n",
    "output = pd.Series(output, index=['precision', 'recall', 'f1', 'accuracy', 'tp', 'fp', 'tn', 'fn']) \n",
    "print(output[['accuracy', 'tp', 'fp', 'tn', 'fn']])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
